{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolov4-tiny without mail\n",
    "import cv2\n",
    "import pywhatkit as pw\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import numpy as np  \n",
    "import pyautogui as pg\n",
    "\n",
    "\n",
    "import base64\n",
    "import simpleaudio as sa\n",
    "\n",
    "\n",
    "# wave_obj = sa.WaveObject.from_wave_file(\"siren-alert-96052.wav\")\n",
    "# set up YOLOv4 net\n",
    "net = cv2.dnn.readNetFromDarknet(\"/home/king/Downloads/yolov4-tiny.cfg\", \"/home/king/Downloads/yolov4-tiny.weights\")\n",
    "classes = []\n",
    "with open(\"/home/king/Downloads/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in [net.getUnconnectedOutLayers()]]\n",
    "\n",
    "\n",
    "# initialize the video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "# set the width of the frames to 640\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "\n",
    "# set the height of the frames to 480\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# set the frame rate to 30 fps\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "\n",
    "mp3_file = \"/home/king/Downloads/siren-alert-96052.mp3\"\n",
    "wav_file = \"/home/king/Downloads/siren-alert-96052.wav\"\n",
    "audio = AudioSegment.from_mp3(mp3_file)\n",
    "\n",
    "# convert the audio to WAV\n",
    "audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "alarm_threshold = 4 # number of frames with a person detected required to trigger the alarm\n",
    "alarm_counter = 0\n",
    "alarm_triggered = False\n",
    "\n",
    "message_sent=False\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    # detect objects using YOLOv4-tiny\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # initialize lists for detected objects\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # process output from YOLOv4-tiny\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # apply non-max suppression to remove redundant overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    indices = np.array(indices)\n",
    "\n",
    "    # draw bounding boxes and labels for the detected objects\n",
    "    for i in indices.flatten():\n",
    "        \n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "        label = classes[class_ids[i]]\n",
    "        if label == 'person':\n",
    "            #play_obj = wave_obj.play()\n",
    "            \n",
    "            cv2.rectangle(frame, tuple(map(int, (x, y))), tuple(map(int, (x + w, y + h))), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, label, (int(x), int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imwrite('human_breach.png', frame)\n",
    "\n",
    "            # if a person is detected, and the message hasn't been sent yet, send the message and set the flag variable to True\n",
    "            if not message_sent:\n",
    "                now=datetime.datetime.now()\n",
    "\n",
    "                formatted_time = \"{}:{}\".format(now.hour,now.minute + 1)\n",
    "\n",
    "                message= \"Human movement detected at {}\\n\".format((now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "                \n",
    "                image = \"human_breach.png\"\n",
    "                pw.sendwhats_image(\"+919458900689\", image,message)\n",
    "\n",
    "               \n",
    "                message_sent = True\n",
    "\n",
    "\n",
    "    # add a timestamp to the frame\n",
    "    cv2.putText(frame, time.strftime(\"%A %d %B %Y %I:%M:%S%p\"), (10, frame.shape[0] - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # add a timestamp to the frame\n",
    "    cv2.putText(frame, time.strftime(\"%A %d %B %Y %I:%M:%S%p\"), (10, frame.shape[0] - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Smart Surveillance System\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "      # if the 'q' key is pressed, stop the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "\n",
    "# release the video capture device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vansh/miniconda3/envs/tf/lib/python3.9/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffprobe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vansh/Documents/hackathon 7.0/final_project.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vansh/Documents/hackathon%207.0/final_project.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m mp3_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/vansh/Downloads/siren-alert-96052.mp3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vansh/Documents/hackathon%207.0/final_project.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m wav_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/vansh/Downloads/siren-alert-96052.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vansh/Documents/hackathon%207.0/final_project.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m audio \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39;49mfrom_mp3(mp3_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vansh/Documents/hackathon%207.0/final_project.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# convert the audio to WAV\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vansh/Documents/hackathon%207.0/final_project.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m audio\u001b[39m.\u001b[39mexport(wav_file, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwav\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pydub/audio_segment.py:796\u001b[0m, in \u001b[0;36mAudioSegment.from_mp3\u001b[0;34m(cls, file, parameters)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_mp3\u001b[39m(\u001b[39mcls\u001b[39m, file, parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 796\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_file(file, \u001b[39m'\u001b[39;49m\u001b[39mmp3\u001b[39;49m\u001b[39m'\u001b[39;49m, parameters\u001b[39m=\u001b[39;49mparameters)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pydub/audio_segment.py:728\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 728\u001b[0m     info \u001b[39m=\u001b[39m mediainfo_json(orig_file, read_ahead_limit\u001b[39m=\u001b[39;49mread_ahead_limit)\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[1;32m    730\u001b[0m     audio_streams \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mstreams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    731\u001b[0m                      \u001b[39mif\u001b[39;00m x[\u001b[39m'\u001b[39m\u001b[39mcodec_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pydub/utils.py:274\u001b[0m, in \u001b[0;36mmediainfo_json\u001b[0;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[1;32m    271\u001b[0m         file\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    273\u001b[0m command \u001b[39m=\u001b[39m [prober, \u001b[39m'\u001b[39m\u001b[39m-of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m command_args\n\u001b[0;32m--> 274\u001b[0m res \u001b[39m=\u001b[39m Popen(command, stdin\u001b[39m=\u001b[39;49mstdin_parameter, stdout\u001b[39m=\u001b[39;49mPIPE, stderr\u001b[39m=\u001b[39;49mPIPE)\n\u001b[1;32m    275\u001b[0m output, stderr \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mstdin_data)\n\u001b[1;32m    276\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    952\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    953\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    954\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    955\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    956\u001b[0m                         errread, errwrite,\n\u001b[1;32m    957\u001b[0m                         restore_signals,\n\u001b[1;32m    958\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    959\u001b[0m                         start_new_session)\n\u001b[1;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/subprocess.py:1837\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1836\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1837\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1838\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffprobe'"
     ]
    }
   ],
   "source": [
    "#FULL PROJECT THE END \n",
    "import cv2\n",
    "import pywhatkit as pw\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np  \n",
    "import camera\n",
    "from pydub import AudioSegment\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import COMMASPACE\n",
    "from email import encoders\n",
    "import base64\n",
    "\n",
    "from_email = 'vsantdasani786@gmail.com'\n",
    "from_email_password ='lcsyqwnnpbhfrxdc'\n",
    "to_email = '9theraphotographer@gmail.com'\n",
    "\n",
    "# set up YOLOv4 net\n",
    "net = cv2.dnn.readNetFromDarknet(\"/home/vansh/Downloads/yolov4-tiny.cfg\", \"/home/vansh/Downloads/yolov4-tiny.weights\")\n",
    "classes = []\n",
    "with open(\"/home/vansh/Downloads/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in [net.getUnconnectedOutLayers()]]\n",
    "\n",
    "\n",
    "# initialize the video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# set the width of the frames to 640\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "\n",
    "# set the height of the frames to 480\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# set the frame rate to 30 fps\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "\n",
    "mp3_file = \"/home/vansh/Downloads/siren-alert-96052.mp3\"\n",
    "wav_file = \"/home/vansh/Downloads/siren-alert-96052.wav\"\n",
    "audio = AudioSegment.from_mp3(mp3_file)\n",
    "\n",
    "# convert the audio to WAV\n",
    "audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "alarm_threshold = 4 # number of frames with a person detected required to trigger the alarm\n",
    "alarm_counter = 0\n",
    "alarm_triggered = False\n",
    "\n",
    "message_sent=False\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    # detect objects using YOLOv4-tiny\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    # initialize lists for detected objects\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # process output from YOLOv4-tiny\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # apply non-max suppression to remove redundant overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    indices = np.array(indices)\n",
    "\n",
    "    # draw bounding boxes and labels for the detected objects\n",
    "    for i in indices.flatten():\n",
    "        \n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "        label = classes[class_ids[i]]\n",
    "        if label == 'person':\n",
    "            cv2.rectangle(frame, tuple(map(int, (x, y))), tuple(map(int, (x + w, y + h))), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, label, (int(x), int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imwrite('human_breach.png', frame)\n",
    "             # Compose the email message\n",
    "            msg = MIMEMultipart()\n",
    "            msg['From'] = from_email\n",
    "            msg['To'] = to_email\n",
    "            msg['Subject'] = \"Person Detected!\"\n",
    "\n",
    "            body = \"A person has been detected. See the attached photo.\"\n",
    "            msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "             # Attach the image to the email\n",
    "            with open('human_breach.png', \"rb\") as f:\n",
    "                part = MIMEBase('application', \"octet-stream\")\n",
    "                part.set_payload(f.read())\n",
    "                encoders.encode_base64(part)\n",
    "                part.add_header('Content-Disposition', 'attachment', filename='human_breach.png')\n",
    "                msg.attach(part)\n",
    "\n",
    "            # Send the email\n",
    "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "            server.starttls()\n",
    "            server.login(from_email, from_email_password)\n",
    "            server.sendmail(from_email, to_email, msg.as_string())\n",
    "            server.quit()\n",
    "            # if a person is detected, and the message hasn't been sent yet, send the message and set the flag variable to True\n",
    "            if not message_sent:\n",
    "                now=datetime.datetime.now()\n",
    "\n",
    "                formatted_time = \"{}:{}\".format(now.hour,now.minute + 1)\n",
    "\n",
    "                message= \"Human movement detected at {}\".format((now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                \n",
    "                \n",
    "                image = \"human_breach.png\"\n",
    "                pw.sendwhats_image(\"+919458900689\", image, message)\n",
    "\n",
    "               \n",
    "                message_sent = True\n",
    "\n",
    "\n",
    "# add a timestamp to the frame\n",
    "    cv2.putText(frame, time.strftime(\"%A %d %B %Y %I:%M:%S%p\"), (10, frame.shape[0] - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Smart Surveillance System\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "      # if the 'q' key is pressed, stop the loop\n",
    "    if key == ord(\"q\"):\n",
    "     os.system.exit()\n",
    "\n",
    "\n",
    "# release the video capture device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55af83e856cc97ea54ed78215be5bf12d8cc8558ad28e8c2642384ebb5c5fded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
